{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning is not that complicated (in Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data set: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8439"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[]\n",
    "documents=[]\n",
    "with open('./reviews.csv') as f:\n",
    "    for line in f:\n",
    "        line_list = line.split('|')\n",
    "        if line_list[3] == '1' or line_list[3] == '2':\n",
    "            labels.append('neg')\n",
    "            documents.append(line_list[4])\n",
    "\n",
    "        elif line_list[3] == '4' or line_list[3] == '5':\n",
    "            labels.append('pos')\n",
    "            documents.append(line_list[4])\n",
    "\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book was a bizarre experience for me. It reads much like a traditional, classic English novel, except with loads of descriptive sex and vulgar words mixed in for shock value. Instead of being shocked, though, I just found it all a bit tiresome and rather silly.  Maybe it was the fact that Lawrence sometimes used words like \"thee\" and \"thy\" and \"dost\" mixed in with modern day vulgarities that added to the overall unintentional humor of it for me, or perhaps it was that the vulgarities were simply used so darned often. In any event, I found myself laughing out loud often. I also found myself cringing. C and F words aside, did anyone tell Lawrence the word \"bowels\" is not particularly appealing? Anyway, I can see why some people felt at the time this was quite simply a trashy romance disguised as literature. It kind of is. Well-written and intelligent, for the most part, but still a bit trashy nonetheless. The story is essentially this: Lady Chatterley's young husband is paralyzed from an injury at war, and is rendered impotent. This leaves the question: can Lady Chatterley be happy in her new marriage without sexual intimacy, or is she excused for seeking physical satisfaction elsewhere with another man? This may have made for a somewhat interesting story (whether you sympathized with her or not) had Lady Chatterley and her husband had a loving friendship or an otherwise soulful or intellectual connection of some kind. But, they didn't. They seemed to have little in common at all. And as such, the book became (for me) merely a story about a woman generally unhappy in her marriage who chooses to have an affair. The fact that her husband is paralyzed becomes almost irrelevant, as it appeared their marriage would have lacked love and passion (physical or emotional) regardless.  While I understand Lawrence was trying to argue that both mind and body must be equally satisfied, particularly in a society Lawrence felt was growing more industrialized and thus emotionally and physically stilted (as was Lady Chatterley's husband), I don't think he did so in the most effective or impressive of ways. I think I had been hoping to read a book about the complexities of what truly defines intimacy, and how a sudden illness or disability can alter a relationship. But that was definitely not this book. That being said, there are somewhat interesting (and yet also at times rather dull) discussions on the state of the social classes and industrialization in post-WWI England, as well as some rather open (if not extremely overly stereotypical) dialogues about the differences in which men and women viewed sex at the time. Vulgar words excluded, the prose is quite lovely in places, and there's no doubt Lawrence can write. But in the end, it felt much to me like nothing more than a book trying way too hard to be provocative. I just could not take it seriously.\n",
      "\n",
      "neg\n",
      "\n",
      "One of my all time best of the best books. This is one author I would so love to meet in real life. This is a true classic novel; I read in high school and studied at uni. Not matter how many times read; it remains a 5â˜… each time. A must read for all readers, imo.\n",
      "\n",
      "pos\n",
      "\n",
      "Mr. Thomas Gradgrind , a very wealthy, former merchant, now retired, only believes in facts, and mathematics, two plus two, is four... facts are important, facts will lift you into prosperity, facts are what to live by, they are the only thing that matters, everything else is worthless ... knowing. He sets up a model school, were the terrorized students, will learn this, ( and other subjects that are unfortunately, also taught) the eminently practical man, teaches his five children at birth ... facts! They fear him, a dictator, at home, his weak minded, sick wife, just looks on, wrapping herself up, to keep warm and complaining of her weariness . But fictitious Coketown , (Manchester) is a dirty, factory town, incessant noises from countless machines, powered by coal, chimneys forever spewing dark gases, polluting the air, thick smoke like a twisting snake high above the atmosphere, moving this way and that, spreading all through the surrounding areas, the filth, the sickness, and early death, to the inhabitants, but the \"hands\" are not relevant, money is, making lots of it, that, and only that. A foul- smelling canal, and even more, a purple river, flows by , the buildings becoming an ugly gray, quickly, the people have to escape to the countryside, to breath fresh healthy air. Travelers going by this place, can only imagine there is a city there, under the black cloud covering, yet they can't see it. Mr. Gradgrind best friend, if there is such an animal, in his circle, is the banker, and manufacturer, Mr. Josiah Bounderby, always telling anyone, within hearing distance, that he himself, rose from the gutter, to become a rich man, no help... he did it alone . Story after story, of his sleeping in the streets, hungry, soiled, without a farthing to his name. Abandoned by the evil, uncaring, widowed mother, brought up by his horrible, drunken grandmother, who beats the child repeatedly . Entertaining, heart-wrenching, you felt for this man, how he suffered greatly in youth, except it's not quite true ...in fact, lies. Louisa, Mr. Gradgrind's oldest and favorite child, is very pretty, the bachelor Bounderby, has eyes for her, when she reaches the proper age of about 20, the fifty- year -old man, asks for her hand in marriage, of course, conveying this fact first, to her father. Louisa says what does it matter, a prisoner in her own home, the girl hasn't seen anything of the world, disaster follows, the couple have nothing in common, what can they talk about? Mrs. Sparsit, her husband's meddling housekeeper, from a good family, hates her. Louisa, flirts with the restless, gentleman, Mr. James Harthouse, who proudly states that he is no good! Still Louisa, only loves her brother, \"The Whelp\", young Thomas, getting money from his sister, gambling, drinking, wasting it all and always coming back for more. The selfish boy, works in the bank for Mr. Bounderby, his now, brother- in- law, when the well runs dry, the drunkard \"finds\" some 150 pounds sterling, inside the bank, not properly being used and sees, that it will be. Implicating an innocent \"hand\", Stephen Blackpool, fired recently by Bounderby, for speaking too much, shunned by the trade union members, for not joining, he walks the streets a lonely man, with an alcoholic wife who deserted him, she still periodically comes back , to sober up, and a sweetheart, that he can't marry too. Mr.Blackpool, seeks work elsewhere, not knowing he's a suspect, in the puzzling crime. The industrial revolution makes some people rich and others sick, but there is no going back , the dye has been cast ...\n",
      "\n",
      "pos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document, label in zip(documents, labels)[:3]:\n",
    "    print document\n",
    "    print label\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number neg: 2193\n"
     ]
    }
   ],
   "source": [
    "print 'number neg:', len([item for item in labels if item == 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number pos: 6246\n"
     ]
    }
   ],
   "source": [
    "print 'number pos:', len([item for item in labels if item == 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform texts into vectors\n",
    "\n",
    "let's use TF-IDF (term frequency, inverse document frequency):\n",
    "\n",
    "- give more weight to words that occur a lot within a document\n",
    "- give less weight to words that occur in many documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8439, 73370) (8439,)\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(documents)\n",
    "y = np.array(labels)\n",
    "\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'pos', 'pos', ..., 'pos', 'pos', 'pos'],\n",
       "      dtype='|S3')"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instantiate classifier\n",
    "\n",
    "naive Bayes:\n",
    "\n",
    "$$probability(spam | document) = probability(document | spam) \\times probability(spam) / probability(document)$$\n",
    "\n",
    "$$ \\approx prob(word_1|spam) \\times prob(word_2|spam) \\times ... \\times prob(word_n|spam) \\times prob(spam)$$\n",
    "\n",
    "\"naive\" = \"wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cross_validation.StratifiedKFold(y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision=[]\n",
    "recall=[]\n",
    "for train, test in cv:\n",
    "    X_train = X[train]\n",
    "    X_test = X[test]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "#     print len(y_train), len(y_test)\n",
    "#     clf = BernoulliNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat = clf.predict(X_test)\n",
    "    p,r,_,_ = metrics.precision_recall_fscore_support(y_test, y_hat)\n",
    "    precision.append(p[1])\n",
    "    recall.append(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.77249022164276404,\n",
       " 0.76329442282749671,\n",
       " 0.75631885936487364,\n",
       " 0.7480053191489362,\n",
       " 0.76290760869565222]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# average precision / recall across k-folds\n",
    "\n",
    "- precision: of predicted negative reviews, how many are actually negative reviews?\n",
    "- recall: of the actual negative reviews, how many are predicted to be negative reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "precision: 0.760603286336 +/- 0.00813531117853\n",
      "recall: 0.924908246597 +/- 0.0208652428031\n"
     ]
    }
   ],
   "source": [
    "print vectorizer\n",
    "print clf\n",
    "print 'precision:',np.average(precision), '+/-', np.std(precision)\n",
    "print 'recall:', np.average(recall), '+/-', np.std(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try on new negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample = 'yuck sickening overblown yawn overuse scrooge unoriginal'\n",
    "neg_sample = vectorizer.transform([neg_sample])\n",
    "#print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99999919e-01,   8.10008685e-08]])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(neg_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sample = 'delightful masterfully accessible bittersweet examines terrific playful'\n",
    "pos_sample = vectorizer.transform([pos_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.18745218e-07,   9.99999881e-01]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(pos_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# most spammy words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-d66f2c7567a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "stop_words = vectorizer.get_stop_words()\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73370"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs=clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73370"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=vectorizer.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-3.5305367330144737, u'dnf'),\n",
       " (-3.2428546605626938, u'overuse'),\n",
       " (-2.9915402322817872, u'coworkers'),\n",
       " (-2.9915402322817872, u'sylvia'),\n",
       " (-2.9915402322817872, u'unoriginal'),\n",
       " (-2.8373895524545283, u'congratulatory'),\n",
       " (-2.8373895524545283, u'faire'),\n",
       " (-2.8373895524545283, u'grossed'),\n",
       " (-2.8373895524545283, u'letdown'),\n",
       " (-2.8373895524545283, u'rebus'),\n",
       " (-2.6550679956605743, u'abortions'),\n",
       " (-2.6550679956605743, u'amateurish'),\n",
       " (-2.6550679956605743, u'anagrams'),\n",
       " (-2.6550679956605743, u'astronaut'),\n",
       " (-2.6550679956605743, u'beide'),\n",
       " (-2.6550679956605743, u'catholics'),\n",
       " (-2.6550679956605743, u'concur'),\n",
       " (-2.6550679956605743, u'couldnt'),\n",
       " (-2.6550679956605743, u'daft'),\n",
       " (-2.6550679956605743, u'irgendwie'),\n",
       " (-2.6550679956605743, u'mistero'),\n",
       " (-2.6550679956605743, u'praiseworthy'),\n",
       " (-2.6550679956605743, u'selflessly'),\n",
       " (-2.6550679956605743, u'slogged'),\n",
       " (-2.6550679956605743, u'storica'),\n",
       " (-2.6550679956605743, u'wannabe'),\n",
       " (-2.6550679956605743, u'yadda'),\n",
       " (-2.5497074800027484, u'jetzt'),\n",
       " (-2.4319244443463646, u'aah'),\n",
       " (-2.4319244443463646, u'aggrandizing'),\n",
       " (-2.4319244443463646, u'aileen'),\n",
       " (-2.4319244443463646, u'albatross'),\n",
       " (-2.4319244443463646, u'aro'),\n",
       " (-2.4319244443463646, u'bekommt'),\n",
       " (-2.4319244443463646, u'bleh'),\n",
       " (-2.4319244443463646, u'champ'),\n",
       " (-2.4319244443463646, u'chided'),\n",
       " (-2.4319244443463646, u'clapping'),\n",
       " (-2.4319244443463646, u'consensual'),\n",
       " (-2.4319244443463646, u'contacted'),\n",
       " (-2.4319244443463646, u'crayon'),\n",
       " (-2.4319244443463646, u'daran'),\n",
       " (-2.4319244443463646, u'daydreaming'),\n",
       " (-2.4319244443463646, u'debbie'),\n",
       " (-2.4319244443463646, u'deswegen'),\n",
       " (-2.4319244443463646, u'dramatische'),\n",
       " (-2.4319244443463646, u'ehrlich'),\n",
       " (-2.4319244443463646, u'ellipses'),\n",
       " (-2.4319244443463646, u'faucet'),\n",
       " (-2.4319244443463646, u'ferris'),\n",
       " (-2.4319244443463646, u'firenze'),\n",
       " (-2.4319244443463646, u'flatmate'),\n",
       " (-2.4319244443463646, u'furchtbar'),\n",
       " (-2.4319244443463646, u'genug'),\n",
       " (-2.4319244443463646, u'glosses'),\n",
       " (-2.4319244443463646, u'gustado'),\n",
       " (-2.4319244443463646, u'hampstead'),\n",
       " (-2.4319244443463646, u'hannity'),\n",
       " (-2.4319244443463646, u'hector'),\n",
       " (-2.4319244443463646, u'heirlooms'),\n",
       " (-2.4319244443463646, u'huff'),\n",
       " (-2.4319244443463646, u'inexcusable'),\n",
       " (-2.4319244443463646, u'juist'),\n",
       " (-2.4319244443463646, u'lehrer'),\n",
       " (-2.4319244443463646, u'leider'),\n",
       " (-2.4319244443463646, u'moeite'),\n",
       " (-2.4319244443463646, u'mormons'),\n",
       " (-2.4319244443463646, u'motorcycles'),\n",
       " (-2.4319244443463646, u'paragons'),\n",
       " (-2.4319244443463646, u'pittore')]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(probs,features))[:70]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](cat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.5919560764999119, u'madame'),\n",
       " (2.5919560764999119, u'delightful'),\n",
       " (2.5919560764999119, u'bovary'),\n",
       " (2.5378888552296361, u'heroines'),\n",
       " (2.3883571212586725, u'nazis'),\n",
       " (2.3883571212586725, u'congo'),\n",
       " (2.3883571212586725, u'coast'),\n",
       " (2.3720966003868922, u'extraordinary'),\n",
       " (2.3555672984356821, u'vonnegut'),\n",
       " (2.3555672984356821, u'terrific'),\n",
       " (2.3555672984356821, u'masterfully'),\n",
       " (2.3555672984356821, u'ease'),\n",
       " (2.3216657467600008, u'infamous'),\n",
       " (2.3216657467600008, u'amazingly'),\n",
       " (2.2865744269487305, u'samuel'),\n",
       " (2.2865744269487305, u'bittersweet'),\n",
       " (2.2502067827778554, u'varied'),\n",
       " (2.2502067827778554, u'playful'),\n",
       " (2.2502067827778554, u'lean'),\n",
       " (2.2502067827778554, u'gem'),\n",
       " (2.2502067827778554, u'condemned'),\n",
       " (2.2124664547950079, u'tim'),\n",
       " (2.2124664547950079, u'occupation'),\n",
       " (2.2124664547950079, u'housekeeper'),\n",
       " (2.2124664547950079, u'hardships'),\n",
       " (2.2124664547950079, u'chaucer'),\n",
       " (2.193048368937907, u'visited'),\n",
       " (2.1732457416417272, u'september'),\n",
       " (2.1732457416417272, u'sensibility'),\n",
       " (2.1732457416417272, u'preserve'),\n",
       " (2.1530430343242077, u'alike'),\n",
       " (2.1324237471214715, u'woke'),\n",
       " (2.1324237471214715, u'rigid'),\n",
       " (2.1324237471214715, u'davies'),\n",
       " (2.0898641327026759, u'resonates'),\n",
       " (2.0898641327026759, u'levin'),\n",
       " (2.0898641327026759, u'imperfect'),\n",
       " (2.0898641327026759, u'histories'),\n",
       " (2.0898641327026759, u'heartbreak'),\n",
       " (2.0898641327026759, u'examines'),\n",
       " (2.0898641327026759, u'divide'),\n",
       " (2.0898641327026759, u'baldwin'),\n",
       " (2.0898641327026759, u'admitted'),\n",
       " (2.0454123701318423, u'profiles'),\n",
       " (2.0454123701318423, u'joys'),\n",
       " (2.0454123701318423, u'intellect'),\n",
       " (2.0454123701318423, u'clare'),\n",
       " (2.0454123701318423, u'365'),\n",
       " (1.9988923544969488, u'wordpress'),\n",
       " (1.9988923544969488, u'unfolding'),\n",
       " (1.9988923544969488, u'spaces'),\n",
       " (1.9988923544969488, u'salem'),\n",
       " (1.9988923544969488, u'rivalry'),\n",
       " (1.9988923544969488, u'rhys'),\n",
       " (1.9988923544969488, u'retain'),\n",
       " (1.9988923544969488, u'rereading'),\n",
       " (1.9988923544969488, u'norms'),\n",
       " (1.9988923544969488, u'imposed'),\n",
       " (1.9988923544969488, u'howl'),\n",
       " (1.9988923544969488, u'egypt'),\n",
       " (1.9988923544969488, u'dresden'),\n",
       " (1.9988923544969488, u'accessible'),\n",
       " (1.9747948029178888, u'2016'),\n",
       " (1.9501021903275166, u'watership'),\n",
       " (1.9501021903275166, u'rosaleen'),\n",
       " (1.9501021903275166, u'poll'),\n",
       " (1.9501021903275166, u'matilda'),\n",
       " (1.9501021903275166, u'ishiguro'),\n",
       " (1.9501021903275166, u'hazel'),\n",
       " (1.9501021903275166, u'germans')]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(probs,features), reverse=True)[:70]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-2933facb82ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneg_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neg_words.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
